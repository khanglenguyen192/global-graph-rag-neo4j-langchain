{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tiktoken\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.vectorstores.neo4j_vector import (\n",
    "    Neo4jVector,\n",
    ")\n",
    "\n",
    "from neo4j import Result\n",
    "from typing import Dict, Any\n",
    "from langchain.chains import RetrievalQA\n",
    "from bert_score import score as bert_score\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5225ec0c680b9c53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_dotenv() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1067775abb141075"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(refresh_schema=True, database=\"neo4j\")    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hàm đếm tokens\n",
    "def num_tokens_from_string(string: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    return len(encoding.encode(string))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b1178dbfc621671"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "  llm=llm, \n",
    "  node_properties=[\"description\"],\n",
    "  relationship_properties=[\"description\"]\n",
    ")\n",
    "\n",
    "def process_text(text: str) -> List[Document]:\n",
    "    doc = Document(page_content=text)\n",
    "    return llm_transformer.convert_to_graph_documents([doc])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c5bb6d64bd2ab49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "588bc5e102b49c36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Process Text data**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf63baa5d34d195"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test\n",
    "sample_text = \"\"\n",
    "with open(\"data/sample_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "sample_doc = process_text(text=sample_text)\n",
    "\n",
    "print(sample_doc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2211b8c80be6702a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    "    sample_doc,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f23b5f1065faae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i in range(1, 64):\n",
    "    chunkDoc = \"\"\n",
    "    chunkFileName = \"p\" + str(i) + \".txt\"\n",
    "    with open(\"data/quydinhdaotaothacsi/\" + chunkFileName, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunkDoc = f.read()\n",
    "    documents.append(chunkDoc)    \n",
    "    chunkDocProcessed = process_text(text=chunkDoc)\n",
    "    graph.add_graph_documents(\n",
    "    chunkDocProcessed,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")\n",
    "    \n",
    "print(len(documents))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad14b4ab16add7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_WORKERS = 2\n",
    "NUM_CHUNK = 63\n",
    "\n",
    "def process_chunk_file(i):\n",
    "    chunkFileName = f\"p{i}.txt\"\n",
    "    file_path = f\"data/quydinhdaotaothacsi/{chunkFileName}\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunkDoc = f.read()\n",
    "    return process_text(text=chunkDoc)\n",
    "\n",
    "graph_documents = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_chunk_file, i)\n",
    "        for i in range(1, NUM_CHUNK + 1)\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        chunkDocProcessed = future.result()\n",
    "        graph_documents.extend(chunkDocProcessed)\n",
    "\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5661544d9586edc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## De-duplication"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ab31085cf8cdc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create and store vector embeddings\n",
    "vector = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    node_label='__Entity__',\n",
    "    text_node_properties=['id', 'description'],\n",
    "    embedding_node_property='embedding'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69ee8ef7d00cc262"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gds = GraphDataScience(\n",
    "    os.environ[\"NEO4J_URI\"],\n",
    "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb392785167b6693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G, result = gds.graph.project(\n",
    "    \"entities\",                   # Graph name\n",
    "    \"__Entity__\",                 # Node projection\n",
    "    \"*\",                          # Relationship projection\n",
    "    nodeProperties=[\"embedding\"]  # Configuration parameters\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66f674512235c498"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_threshold = 0.95\n",
    "\n",
    "gds.knn.mutate(\n",
    "  G,\n",
    "  nodeProperties=['embedding'],\n",
    "  mutateRelationshipType= 'SIMILAR',\n",
    "  mutateProperty= 'score',\n",
    "  similarityCutoff=similarity_threshold\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79c30669ec64e650"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gds.wcc.write(\n",
    "    G,\n",
    "    writeProperty=\"wcc\",\n",
    "    relationshipTypes=[\"SIMILAR\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c2ab11389a3b6a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_edit_distance = 3\n",
    "potential_duplicate_candidates = graph.query(\n",
    "    \"\"\"MATCH (e:`__Entity__`)\n",
    "    WHERE size(e.id) > 3 // longer than 3 characters\n",
    "    WITH e.wcc AS community, collect(e) AS nodes, count(*) AS count\n",
    "    WHERE count > 1\n",
    "    UNWIND nodes AS node\n",
    "    // Add text distance\n",
    "    WITH distinct\n",
    "      [n IN nodes WHERE apoc.text.distance(toLower(node.id), toLower(n.id)) < $distance \n",
    "                  OR node.id CONTAINS n.id | n.id] AS intermediate_results\n",
    "    WHERE size(intermediate_results) > 1\n",
    "    WITH collect(intermediate_results) AS results\n",
    "    // combine groups together if they share elements\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult\n",
    "    \"\"\", params={'distance': word_edit_distance})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae41acfc7c699c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Xác định các thực thể trùng lặp trong danh sách và quyết định thực thể nào trong số chúng nên được hợp nhất.\n",
    "Các thực thể có thể hơi khác nhau về định dạng hoặc nội dung, nhưng về cơ bản đều đề cập đến cùng một thứ. Sử dụng các kỹ năng phân tích của bạn để xác định các bản sao.\n",
    "\n",
    "Sau đây là các quy tắc để xác định các bản sao:\n",
    "1. Các thực thể có sự khác biệt nhỏ về kiểu chữ nên được coi là bản sao.\n",
    "2. Các thực thể có định dạng khác nhau nhưng cùng nội dung nên được coi là bản sao.\n",
    "3. Các thực thể đề cập đến cùng một đối tượng hoặc khái niệm trong thế giới thực, ngay cả khi được mô tả khác nhau, nên được coi là bản sao.\n",
    "4. Nếu nó đề cập đến các số, ngày hoặc sản phẩm khác nhau, thì không được hợp nhất kết quả\n",
    "\"\"\"\n",
    "user_template = \"\"\"\n",
    "Dưới đây là danh sách các thực thể cần xử lý:\n",
    "{entities}\n",
    "\n",
    "Xác định các mục trùng lặp, hợp nhất chúng và cung cấp danh sách đã hợp nhất.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e513c43b9a52d7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DuplicateEntities(BaseModel):\n",
    "    entities: List[str] = Field(\n",
    "        description=\"Các thực thể đại diện cho cùng một đối tượng hoặc thực thể trong thế giới thực và cần được hợp nhất\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Disambiguate(BaseModel):\n",
    "    merge_entities: Optional[List[DuplicateEntities]] = Field(\n",
    "        description=\"Danh sách các thực thể đại diện cho cùng một đối tượng hoặc thực thể trong thế giới thực và cần được hợp nhất\"\n",
    "    )\n",
    "\n",
    "\n",
    "extraction_llm = ChatOpenAI(model_name=\"gpt-4o\").with_structured_output(\n",
    "    Disambiguate\n",
    ")\n",
    "\n",
    "extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            user_template,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579614d75cdff598"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extraction_chain = extraction_prompt | extraction_llm\n",
    "\n",
    "def entity_resolution(entities: List[str]) -> Optional[List[List[str]]]:\n",
    "    return [\n",
    "        el.entities\n",
    "        for el in extraction_chain.invoke({\"entities\": entities}).merge_entities\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f78c1955ee2c9c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_entities = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submitting all tasks and creating a list of future objects\n",
    "    futures = [\n",
    "        executor.submit(entity_resolution, el['combinedResult'])\n",
    "        for el in potential_duplicate_candidates\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Processing documents\"\n",
    "    ):\n",
    "        to_merge = future.result()\n",
    "        if to_merge is not None:\n",
    "            merged_entities.extend(to_merge)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad8ba7aa0622e81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(merged_entities)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1094f89645393a7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "UNWIND $data AS candidates\n",
    "CALL {\n",
    "  WITH candidates\n",
    "  MATCH (e:__Entity__) WHERE e.id IN candidates\n",
    "  RETURN collect(e) AS nodes\n",
    "}\n",
    "CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "    description:'combine',\n",
    "    `.*`: 'discard'\n",
    "}})\n",
    "YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\", params={\"data\": merged_entities})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9e925ae79092d1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constructing and Summarizing Communities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7126832d5009d90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G, result = gds.graph.project(\n",
    "    \"communities\",  #  Graph name\n",
    "    \"__Entity__\",  #  Node projection\n",
    "    {\n",
    "        \"_ALL_\": {\n",
    "            \"type\": \"*\",\n",
    "            \"orientation\": \"UNDIRECTED\",\n",
    "            \"properties\": {\"weight\": {\"property\": \"*\", \"aggregation\": \"COUNT\"}},\n",
    "        }\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e30b9abe0a6af3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wcc = gds.wcc.stats(G)\n",
    "print(f\"Component count: {wcc['componentCount']}\")\n",
    "print(f\"Component distribution: {wcc['componentDistribution']}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "453cb63b2e7fb022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gds.leiden.write(\n",
    "    G,\n",
    "    writeProperty=\"communities\",\n",
    "    includeIntermediateCommunities=True,\n",
    "    relationshipWeightProperty=\"weight\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bee38996a601c41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "MATCH (e:`__Entity__`)\n",
    "UNWIND range(0, size(e.communities) - 1 , 1) AS index\n",
    "CALL {\n",
    "  WITH e, index\n",
    "  WITH e, index\n",
    "  WHERE index = 0\n",
    "  MERGE (c:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
    "  ON CREATE SET c.level = index\n",
    "  MERGE (e)-[:IN_COMMUNITY]->(c)\n",
    "  RETURN count(*) AS count_0\n",
    "}\n",
    "CALL {\n",
    "  WITH e, index\n",
    "  WITH e, index\n",
    "  WHERE index > 0\n",
    "  MERGE (current:`__Community__` {id: toString(index) + '-' + toString(e.communities[index])})\n",
    "  ON CREATE SET current.level = index\n",
    "  MERGE (previous:`__Community__` {id: toString(index - 1) + '-' + toString(e.communities[index - 1])})\n",
    "  ON CREATE SET previous.level = index - 1\n",
    "  MERGE (previous)-[:IN_COMMUNITY]->(current)\n",
    "  RETURN count(*) AS count_1\n",
    "}\n",
    "RETURN count(*)\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2fd9f3e289a032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(:__Entity__)<-[:MENTIONS]-(d:Document)\n",
    "WITH c, count(distinct d) AS rank\n",
    "SET c.community_rank = rank;\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd4f2fab60df303"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "community_size = graph.query(\n",
    "    \"\"\"\n",
    "MATCH (c:__Community__)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
    "WITH c, count(distinct e) AS entities\n",
    "RETURN split(c.id, '-')[0] AS level, entities\n",
    "\"\"\"\n",
    ")\n",
    "community_size_df = pd.DataFrame.from_records(community_size)\n",
    "percentiles_data = []\n",
    "for level in community_size_df[\"level\"].unique():\n",
    "    subset = community_size_df[community_size_df[\"level\"] == level][\"entities\"]\n",
    "    num_communities = len(subset)\n",
    "    percentiles = np.percentile(subset, [25, 50, 75, 90, 99])\n",
    "    percentiles_data.append(\n",
    "        [\n",
    "            level,\n",
    "            num_communities,\n",
    "            percentiles[0],\n",
    "            percentiles[1],\n",
    "            percentiles[2],\n",
    "            percentiles[3],\n",
    "            percentiles[4],\n",
    "            max(subset)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Create a DataFrame with the percentiles\n",
    "percentiles_df = pd.DataFrame(\n",
    "    percentiles_data,\n",
    "    columns=[\n",
    "        \"Level\",\n",
    "        \"Number of communities\",\n",
    "        \"25th Percentile\",\n",
    "        \"50th Percentile\",\n",
    "        \"75th Percentile\",\n",
    "        \"90th Percentile\",\n",
    "        \"99th Percentile\",\n",
    "        \"Max\"\n",
    "    ],\n",
    ")\n",
    "percentiles_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e01fceacb626e70c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "community_info = graph.query(\"\"\"\n",
    "MATCH (c:`__Community__`)<-[:IN_COMMUNITY*]-(e:__Entity__)\n",
    "WHERE c.level IN [0,1,4]\n",
    "WITH c, collect(e ) AS nodes\n",
    "WHERE size(nodes) > 1\n",
    "CALL apoc.path.subgraphAll(nodes[0], {\n",
    " whitelistNodes:nodes\n",
    "})\n",
    "YIELD relationships\n",
    "RETURN c.id AS communityId, \n",
    "       [n in nodes | {id: n.id, description: n.description, type: [el in labels(n) WHERE el <> '__Entity__'][0]}] AS nodes,\n",
    "       [r in relationships | {start: startNode(r).id, type: type(r), end: endNode(r).id, description: r.description}] AS rels\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a8fe6759325a2c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "community_template = \"\"\"Dựa trên các nút và mối quan hệ được cung cấp thuộc về cùng một cộng đồng đồ thị, tạo bản tóm tắt bằng ngôn ngữ tự nhiên về thông tin được cung cấp:\n",
    "{community_info}\n",
    "\n",
    "Tóm tăt:\"\"\"\n",
    "\n",
    "community_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Với một bộ ba đầu vào, tạo ra bản tóm tắt thông tin. Không có phần mở đầu.\",\n",
    "        ),\n",
    "        (\"human\", community_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "community_chain = community_prompt | llm | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e28703c245ba75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_string(data):\n",
    "    nodes_str = \"Nodes are:n\"\n",
    "    for node in data['nodes']:\n",
    "        node_id = node['id']\n",
    "        node_type = node['type']\n",
    "        if 'description' in node and node['description']:\n",
    "            node_description = f\", description: {node['description']}\"\n",
    "        else:\n",
    "            node_description = \"\"\n",
    "        nodes_str += f\"id: {node_id}, type: {node_type}{node_description}n\"\n",
    "\n",
    "    rels_str = \"Relationships are:n\"\n",
    "    for rel in data['rels']:\n",
    "        start = rel['start']\n",
    "        end = rel['end']\n",
    "        rel_type = rel['type']\n",
    "        if 'description' in rel and rel['description']:\n",
    "            description = f\", description: {rel['description']}\"\n",
    "        else:\n",
    "            description = \"\"\n",
    "        rels_str += f\"({start})-[:{rel_type}]->({end}){description}n\"\n",
    "\n",
    "    return nodes_str + \"n\" + rels_str\n",
    "\n",
    "def process_community(community):\n",
    "    stringify_info = prepare_string(community)\n",
    "    summary = community_chain.invoke({'community_info': stringify_info})\n",
    "    return {\"community\": community['communityId'], \"summary\": summary}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a35501304968e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summaries = []\n",
    "MAX_WORKERS = 2\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_community, community): community for community in community_info}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing communities\"):\n",
    "        summaries.append(future.result())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4966f22bc7b6f3b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(summaries)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e5ebcae11f95d82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph.query(\"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (c:__Community__ {id:row.community})\n",
    "SET c.summary = row.summary\n",
    "\"\"\", params={\"data\": summaries})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "573d6a4743c070e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local Retrieval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c1db7f066865cb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "\n",
    "def create_fulltext_index(tx):\n",
    "    query = '''\n",
    "    CREATE FULLTEXT INDEX `fulltext_entity_id` \n",
    "    FOR (n:__Entity__) \n",
    "    ON EACH [n.id];\n",
    "    '''\n",
    "    tx.run(query)\n",
    "\n",
    "# Function to execute the query\n",
    "def create_index():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_fulltext_index)\n",
    "        print(\"Fulltext index created successfully.\")\n",
    "\n",
    "# Call the function to create the index\n",
    "try:\n",
    "    create_index()\n",
    "except:\n",
    "    print(\"Fulltext index created failed.\")\n",
    "    pass\n",
    "\n",
    "# Close the driver connection\n",
    "driver.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd051ba19e3658bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "** Extract entities from question **"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73bc21a8c49d7f9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Tất cả các thực thể trong văn bản\",\n",
    "    )\n",
    "\n",
    "extract_entities_from_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Trích xuất thực thể từ văn bản.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Sử dụng định dạng đã cho để trích xuất thông tin các thực thể từ những nội dung sau, trả kết quả dưới dạng danh sách, nếu là một thực thể nhiều thông tin, hoặc là một cụm danh từ tiếng việt , hãy chia ra thành nhiều thực thể con\"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = extract_entities_from_question_prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "def extract_entities_from_question(question):\n",
    "    entity_chain.invoke(input=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "956f46b3c7b5a322"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local query\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7604ef328d2dc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_LOCAL_QUERY = \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\"\n",
    "LOCAL_QUERY_WITH_DESSCRIPTION = \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN\n",
    "                node.id + ' (desc: ' + coalesce(node.description, '') + ')' + ' - ' +\n",
    "                type(r) + ' -> ' +\n",
    "                neighbor.id + ' (desc: ' + coalesce(neighbor.description, '') + ')' AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN\n",
    "                neighbor.id + ' (desc: ' + coalesce(neighbor.description, '') + ')' + ' - ' +\n",
    "                type(r) + ' -> ' +\n",
    "                node.id + ' (desc: ' + coalesce(node.description, '') + ')' AS output\n",
    "            }\n",
    "            RETURN output LIMIT 100\n",
    "            \"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea45957d6d597bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fulltext index query\n",
    "def graph_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke(question)\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            LOCAL_QUERY_WITH_DESSCRIPTION,\n",
    "            {\"query\": entity},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c46241cd9fe243"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sử dụng OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    index_name=\"embedding\",\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "\n",
    "vector_retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "vector_graph_chain = RetrievalQA.from_chain_type(\n",
    "        llm, \n",
    "        retriever = vector_retriever, \n",
    "        verbose=False,\n",
    "        return_source_documents=False,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f11d06987f4b37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def full_retriever(question: str):\n",
    "    graph_data = graph_retriever(question)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
    "    final_data = f\"\"\"Graph data:\n",
    "    {graph_data}\n",
    "    vector data:\n",
    "    {\"#Document \". join(vector_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f461b2ebc847ccd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_retriever(\"Các hình thức thi kết thúc môn học là gì?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e0dee71c8f6242"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "template = \"\"\"Trả lời câu hỏi dựa trên ngữ cảnh được cung cấp, sẽ có thông tin embedding trong ngữ cảnh:\n",
    "{context}\n",
    "Thông tin trong desc là mô tả của thực thể trong ngữ cảnh. Có một số quan hệ được mô tả bằng tiếng Anh, hãy thêm chúng vào ngữ cảnh.\n",
    "Câu hỏi: {question}\n",
    "Sử dụng ngôn ngữ tự nhiên và trả lời ngắn gọn. Không thêm vào các thông tin như có liên quan đến cộng đồng...\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "        {\n",
    "            \"context\": full_retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def graph_answer(input):\n",
    "    return chain.invoke(input=input)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59b31e07e9976e03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_answer(\"Các môn học thuộc khối kiến thức bổ sung là gì?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c3439b57ab98875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global Retrieval\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d732c0a0246f0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def db_query(cypher: str, params: Dict[str, Any] = {}) -> pd.DataFrame:\n",
    "    \"\"\"Executes a Cypher statement and returns a DataFrame\"\"\"\n",
    "    return driver.execute_query(\n",
    "        cypher, parameters_=params, result_transformer_=Result.to_df\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98f96fd0ad5402c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAP_SYSTEM_PROMPT = \"\"\"\n",
    "---Vai trò---\n",
    "\n",
    "Bạn là một trợ lý hữu ích, phản hồi các câu hỏi liên quan đến dữ liệu trong các bảng được cung cấp.\n",
    "\n",
    "\n",
    "---Mục tiêu---\n",
    "\n",
    "Tạo ra một phản hồi gồm danh sách các điểm chính nhằm trả lời câu hỏi của người dùng, tóm tắt tất cả thông tin liên quan trong các bảng dữ liệu đầu vào.\n",
    "\n",
    "Bạn nên sử dụng dữ liệu được cung cấp trong các bảng dữ liệu dưới đây làm bối cảnh chính để tạo phản hồi.\n",
    "Nếu bạn không biết câu trả lời hoặc nếu các bảng dữ liệu đầu vào không chứa đủ thông tin để đưa ra câu trả lời, chỉ cần nói như vậy. Không được bịa ra bất kỳ thông tin nào.\n",
    "\n",
    "Mỗi điểm chính trong phản hồi phải bao gồm các yếu tố sau:\n",
    "- Mô tả: Một mô tả đầy đủ về điểm đó.\n",
    "- Điểm quan trọng: Một số nguyên trong khoảng từ 0-100 thể hiện mức độ quan trọng của điểm đó trong việc trả lời câu hỏi của người dùng. Phản hồi kiểu “Tôi không biết” sẽ có điểm là 0.\n",
    "\n",
    "Phản hồi phải được định dạng JSON như sau:\n",
    "{{\n",
    "    \"points\": [\n",
    "        {{\"description\": \"Mô tả điểm 1 [Dữ liệu: Báo cáo (các mã báo cáo)]\", \"score\": giá_trị_điểm}},\n",
    "        {{\"description\": \"Mô tả điểm 2 [Dữ liệu: Báo cáo (các mã báo cáo)]\", \"score\": giá_trị_điểm}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Phản hồi phải giữ nguyên ý nghĩa gốc và cách sử dụng các động từ tình thái như “sẽ”, “có thể” .\n",
    "\n",
    "Các điểm có dữ liệu hỗ trợ nên liệt kê các báo cáo liên quan làm tài liệu tham khảo như sau:\n",
    "\"Đây là một câu ví dụ được hỗ trợ bởi các tài liệu tham khảo [Dữ liệu: Báo cáo (các mã báo cáo)]\"\n",
    "\n",
    "**Không liệt kê nhiều hơn 5 mã báo cáo trong một tài liệu tham khảo**. Thay vào đó, hãy liệt kê 5 mã báo cáo liên quan nhất và thêm “+more” để cho biết còn nhiều hơn nữa.\n",
    "\n",
    "Ví dụ:\n",
    "\"Người X là chủ sở hữu của Công ty Y và bị cáo buộc nhiều sai phạm [Dữ liệu: Báo cáo (2, 7, 64, 46, 34, ...)]. Anh ta cũng là CEO của công ty X [Dữ liệu: Báo cáo (1, 3)]\"\n",
    "\n",
    "trong đó 1, 2, 3, 7, 34, 46, và 64 là mã (không phải chỉ số) của các báo cáo dữ liệu liên quan trong bảng được cung cấp.\n",
    "\n",
    "Không bao gồm thông tin nếu không có bằng chứng hỗ trợ từ dữ liệu.\n",
    "\n",
    "---Các bảng dữ liệu---\n",
    "\n",
    "{context_data}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            MAP_SYSTEM_PROMPT,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_chain = map_prompt | llm | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c88c3545053fbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REDUCE_SYSTEM_PROMPT = \"\"\"\n",
    "---Vai trò---\n",
    "\n",
    "Bạn là một trợ lý hữu ích, trả lời các câu hỏi về một tập dữ liệu bằng cách tổng hợp các quan điểm từ nhiều nhà phân tích khác nhau.\n",
    "\n",
    "\n",
    "---Mục tiêu---\n",
    "\n",
    "Tạo ra một phản hồi với độ dài và định dạng mục tiêu nhằm trả lời câu hỏi của người dùng, đồng thời tóm tắt tất cả các báo cáo từ nhiều nhà phân tích, mỗi người tập trung vào các phần khác nhau của tập dữ liệu.\n",
    "\n",
    "Lưu ý rằng các báo cáo của nhà phân tích được cung cấp dưới đây **được sắp xếp theo thứ tự quan trọng giảm dần**.\n",
    "\n",
    "Nếu bạn không biết câu trả lời hoặc nếu các báo cáo được cung cấp không chứa đủ thông tin để đưa ra câu trả lời, chỉ cần nói như vậy. Không được bịa ra bất kỳ thông tin nào.\n",
    "\n",
    "Phản hồi cuối cùng nên loại bỏ tất cả thông tin không liên quan từ các báo cáo của nhà phân tích và tổng hợp thông tin đã được làm sạch thành một câu trả lời toàn diện, giải thích tất cả các điểm chính và các hàm ý, phù hợp với độ dài và định dạng của phản hồi.\n",
    "\n",
    "Hãy thêm các phần và bình luận vào phản hồi nếu phù hợp với độ dài và định dạng. Định dạng phản hồi bằng **markdown**.\n",
    "\n",
    "Phản hồi phải giữ nguyên ý nghĩa gốc và cách sử dụng các động từ tình thái như “sẽ”, “có thể”.\n",
    "\n",
    "Phản hồi cũng phải giữ lại tất cả các tham chiếu dữ liệu đã được nêu trong các báo cáo của nhà phân tích, nhưng **không được đề cập đến vai trò của các nhà phân tích trong quá trình phân tích**.\n",
    "\n",
    "**Không liệt kê nhiều hơn 5 mã báo cáo trong một tham chiếu**. \n",
    "\n",
    "\n",
    "---Độ dài và định dạng phản hồi mục tiêu---\n",
    "\n",
    "{response_type}\n",
    "\n",
    "\n",
    "---Báo cáo của các nhà phân tích---\n",
    "\n",
    "{report_data}\n",
    "\n",
    "Hãy thêm các phần và bình luận vào phản hồi nếu phù hợp với độ dài và định dạng. Định dạng phản hồi bằng markdown.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            REDUCE_SYSTEM_PROMPT,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "reduce_chain = reduce_prompt | llm | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd0dd2e8ac9c9570"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "community_embedding = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    node_label=\"__Community__\",\n",
    "    text_node_properties=[\"summary\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    index_name=\"fulltext_community_summary\",\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "\n",
    "vector_community_retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ff9935708046d73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def global_retriever(query: str, level: int = 0, response_type: str = \"nhiều đoạn văn bản\") -> str:\n",
    "    \n",
    "    # community_data = graph.query(\n",
    "    #     \"\"\"\n",
    "    #     MATCH (c:__Community__)\n",
    "    #     WHERE c.level = $level and c.summary is not null\n",
    "    #     RETURN c.summary AS output limit 20\n",
    "    #     \"\"\",\n",
    "    #     params={\"level\": level},\n",
    "    # )\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    community_data = graph.query(\n",
    "        \"\"\"\n",
    "        WITH $query_embedding AS queryEmbedding\n",
    "        MATCH (c:__Community__)\n",
    "        WHERE c.embedding IS NOT NULL and (c.level = 0 or c.level = 1)\n",
    "        WITH c, vector.similarity.cosine(queryEmbedding, c.embedding) AS score\n",
    "        RETURN c.summary AS output\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 10\n",
    "        \"\"\",\n",
    "        params={\"query_embedding\": query_embedding},\n",
    "    )\n",
    "    \n",
    "    def process_community(community):\n",
    "        return map_chain.invoke({\"question\": query, \"context_data\": community[\"output\"]})\n",
    "\n",
    "    # Dùng ThreadPoolExecutor để chạy song song\n",
    "    intermediate_results = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # có thể chỉnh max_workers tuỳ môi trường\n",
    "        futures = [executor.submit(process_community, comm) for comm in community_data]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Processing communities\"):\n",
    "            intermediate_results.append(f.result())\n",
    "\n",
    "    final_response = reduce_chain.invoke(\n",
    "        {\n",
    "            \"report_data\": intermediate_results,\n",
    "            \"question\": query,\n",
    "            \"response_type\": response_type,\n",
    "        }\n",
    "    )\n",
    "    return final_response\n",
    "\n",
    "def global_answer(question: str):\n",
    "    return global_retriever(query=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31a9592e950c4968"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(global_answer(\"Tóm tắt thông tin về chương trình đào tạo thạc sĩ?\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e9c6f3eadf74e48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Determine query type**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a51f8d71587bc27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def should_use_global_query(question: str) -> bool:\n",
    "    \"\"\"\n",
    "    Xác định xem câu hỏi có nên dùng global query không\n",
    "    Dựa trên các từ khóa mang tính tổng quát/khái quát\n",
    "    \"\"\"\n",
    "    keywords = [\n",
    "    # Các cụm từ phổ biến\n",
    "    \"tóm tắt\", \"tổng quát\", \"khái quát\", \"tổng quan\",\n",
    "    \"đại khái\", \"tóm gọn\", \"nhìn chung\", \"mô tả chung\",\n",
    "    \"toàn cảnh\", \"bao quát\", \"khái lược\", \"phác thảo\",\n",
    "    \"sơ lược\", \"nội dung chính\", \"nội dung tổng quát\",\n",
    "    \"tổng hợp\", \"hệ thống lại\", \"cái nhìn tổng thể\",\n",
    "    \"bức tranh chung\", \"góc nhìn chung\", \"cấu trúc chung\",\n",
    "    \"nói chung\", \"chủ đề chính\", \"cấu trúc tổng quát\",\n",
    "    \"gợi ý chung\",\n",
    "    \"nội dung cốt lõi\", \"trình bày ngắn gọn\", \"nền tảng chung\",\n",
    "    \"khái niệm tổng thể\", \"cốt lõi là gì\", \"giới thiệu tổng quát\",\n",
    "    \"giới thiệu sơ lược\", \"giới thiệu chung\", \"đại cương\",\n",
    "    \"khung chương trình\", \"khung nội dung\", \"mục tiêu chung\",\n",
    "    \"mục tiêu tổng thể\", \"định hướng chung\", \"điểm nổi bật\",\n",
    "    \"điểm chính\", \"ý chính\", \"trọng tâm\", \"xương sống\", \"nền tảng\",\n",
    "    \"toàn thể\", \"tổng thể\", \"tầm nhìn chung\", \"mức độ khái quát\",\n",
    "    \"ngắn gọn lại\", \"khái quát lại\", \"sơ đồ hóa\", \"liệt kê tổng quan\",\n",
    "    \"bản đồ khái niệm\", \"trình bày đại cương\", \"trình bày sơ lược\"\n",
    "]\n",
    "    normalized_question = question.lower()\n",
    "    return any(kw in normalized_question for kw in keywords)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf78a8fc22645b97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "should_use_global_query(\"Tóm tắt thông tin về chương trình đào tạo?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "395209163f9b5ebc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "should_use_global_query(\"Điều kiện Đăng ký môn học gồm những gì?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec54cffff72b1f70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate Answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7bc27be5715287e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Graph RAG**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "525e099c2d57b834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_graph_rag_answers_from_csv(csv_path: str, output_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    real_answers = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            result = graph_answer(row[\"question\"])\n",
    "            real_answers.append(result)\n",
    "        except Exception as e:\n",
    "            real_answers.append(f\"ERROR: {e}\")\n",
    "\n",
    "    df[\"real_answer\"] = real_answers\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e0f935207ec886"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_graph_rag_answers_from_csv(\"evaluation_data/Quy_dinh_dao_tao_Thac_si.csv\", \"evaluation_data/out_put_Quy_dinh_dao_tao_Thac_si.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b693350dc0f030d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_llm_answers_from_csv(csv_path: str, output_path: str):\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    real_answers = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            result = llm.invoke(row[\"question\"])\n",
    "            real_answers.append(result.content)\n",
    "        except Exception as e:\n",
    "            real_answers.append(f\"ERROR: {e}\")\n",
    "\n",
    "    df[\"gpt_answer\"] = real_answers\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d382462aad0d33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_llm_answers_from_csv(\"evaluation_data/Quy_dinh_dao_tao_Thac_si.csv\", \"evaluation_data/out_put_Quy_dinh_dao_tao_Thac_si_llm.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa286f9a71aad0f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7fba7c5675800da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_qa_response(prediction, ground_truth, lang='vi'):\n",
    "    \"\"\"\n",
    "    Đánh giá độ chính xác câu trả lời bằng BERTScore.\n",
    "    \n",
    "    Args:\n",
    "        prediction (str): câu trả lời từ mô hình\n",
    "        ground_truth (str): câu trả lời đúng\n",
    "        lang (str): ngôn ngữ\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'exact_match': ..., 'bertscore': ...}\n",
    "    \"\"\"\n",
    "    P, R, F1 = bert_score([prediction], [ground_truth], lang=lang, rescale_with_baseline=False)\n",
    "    \n",
    "    return {\n",
    "        'bertscore_precision': round(P[0].item(), 4), # Trung bình độ tương đồng của mỗi token trong câu trả lời mô hình với token gần nhất trong đáp án chuẩn.\n",
    "        'bertscore_recall': round(R[0].item(), 4), # Trung bình độ tương đồng của mỗi token trong đáp án chuẩn với token gần nhất trong câu trả lời mô hình.\n",
    "        'bertscore_f1': round(F1[0].item(), 4) # Trung bình hài hòa giữa Precision và Recall.\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e6c2333573ae7cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_bert_score(file_path, output_path):\n",
    "    # Đọc dữ liệu từ file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Tính BERTScore cho từng dòng\n",
    "    results = df.apply(lambda row: evaluate_qa_response(row['answer'], row['real_answer']), axis=1)\n",
    "    results_df = pd.DataFrame(results.tolist())\n",
    "    \n",
    "    # Ghép kết quả vào DataFrame gốc\n",
    "    df = pd.concat([df, results_df], axis=1)\n",
    "    \n",
    "    # Tính điểm trung bình toàn bộ\n",
    "    print(\"Trung bình BERTScore:\")\n",
    "    print(\"Precision:\", df['bertscore_precision'].mean())\n",
    "    print(\"Recall   :\", df['bertscore_recall'].mean())\n",
    "    print(\"F1       :\", df['bertscore_f1'].mean())\n",
    "    \n",
    "    # Ghi ra file nếu muốn lưu lại\n",
    "    df.to_csv(output_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e1d4d17ca81989c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_bert_score(\"evaluation_data/out_put_Quy_dinh_dao_tao_Thac_si_llm.csv\", \"evaluation_data/output_with_bertscore_llm.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5d0b6e3da56d0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UI Demo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8599ea52cc4914c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Canvas, Frame, Scrollbar\n",
    "\n",
    "# ==== GUI Functions ====\n",
    "\n",
    "def send_message():\n",
    "    user_input = entry.get()\n",
    "    if user_input.strip():\n",
    "        create_bubble(user_input, sender=\"user\")\n",
    "        entry.delete(0, tk.END)\n",
    "\n",
    "        # Hiện loading và giữ lại label\n",
    "        loading_label = create_bubble(\"Đang tạo câu trả lời...\", sender=\"bot\", return_label=True)\n",
    "\n",
    "        def generate_and_replace():\n",
    "            answer = global_answer(user_input) if should_use_global_query(user_input) else graph_answer(user_input)\n",
    "            update_bubble(loading_label, answer)\n",
    "\n",
    "        root.after(100, generate_and_replace)\n",
    "\n",
    "def create_bubble(text, sender=\"bot\", return_label=False):\n",
    "    bubble_frame = Frame(message_frame, bg=\"#F0F0F0\")\n",
    "\n",
    "    msg_label = tk.Label(\n",
    "        bubble_frame,\n",
    "        text=text,\n",
    "        wraplength=400,\n",
    "        justify=\"left\",\n",
    "        font=(\"Arial\", 12),\n",
    "        padx=12,\n",
    "        pady=6,\n",
    "        bd=0\n",
    "    )\n",
    "\n",
    "    if sender == \"user\":\n",
    "        msg_label.config(bg=\"#5C6BC0\", fg=\"white\")\n",
    "        msg_label.pack(side=\"right\", padx=5, pady=4)\n",
    "        bubble_frame.pack(anchor=\"e\", fill=\"x\", padx=(50, 10), pady=4)\n",
    "    else:\n",
    "        msg_label.config(bg=\"#ffffff\", fg=\"black\")\n",
    "        msg_label.pack(side=\"left\", padx=5, pady=4)\n",
    "        bubble_frame.pack(anchor=\"w\", fill=\"x\", padx=(10, 50), pady=4)\n",
    "\n",
    "    canvas.update_idletasks()\n",
    "    canvas.yview_moveto(1.0)\n",
    "\n",
    "    return msg_label if return_label else None\n",
    "\n",
    "def update_bubble(label, new_text):\n",
    "    label.config(text=new_text)\n",
    "    canvas.update_idletasks()\n",
    "    canvas.yview_moveto(1.0)\n",
    "\n",
    "# ==== Main window ====\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot Demo UI\")\n",
    "root.geometry(\"600x650\")\n",
    "root.configure(bg=\"#F0F0F0\")\n",
    "\n",
    "# Root grid config\n",
    "root.rowconfigure(0, weight=1)\n",
    "root.rowconfigure(1, weight=0)\n",
    "root.columnconfigure(0, weight=1)\n",
    "root.columnconfigure(1, weight=0)\n",
    "\n",
    "# ==== Display Area ====\n",
    "display_frame = Frame(root, bg=\"#F0F0F0\")\n",
    "display_frame.grid(row=0, column=0, columnspan=2, sticky=\"nsew\")\n",
    "\n",
    "canvas = Canvas(display_frame, bg=\"#F0F0F0\", highlightthickness=0)\n",
    "canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "scrollbar = Scrollbar(root, command=canvas.yview)\n",
    "scrollbar.grid(row=0, column=2, sticky=\"ns\")\n",
    "canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "scrollable_frame = Frame(canvas, bg=\"#F0F0F0\")\n",
    "message_frame = Frame(scrollable_frame, bg=\"#F0F0F0\")\n",
    "\n",
    "canvas_window = canvas.create_window((0, 0), window=scrollable_frame, anchor='nw')\n",
    "message_frame.pack(fill=\"both\", expand=True)\n",
    "\n",
    "# Buộc width của scrollable_frame = width của canvas\n",
    "def resize_canvas(event):\n",
    "    canvas.itemconfig(canvas_window, width=event.width)\n",
    "\n",
    "canvas.bind(\"<Configure>\", resize_canvas)\n",
    "\n",
    "scrollable_frame.bind(\"<Configure>\", lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\")))\n",
    "\n",
    "# ==== Input Area ====\n",
    "entry_frame = Frame(root, bg=\"#FFFFFF\", bd=1)\n",
    "entry_frame.grid(row=1, column=0, columnspan=2, sticky=\"ew\")\n",
    "entry_frame.columnconfigure(0, weight=1)\n",
    "\n",
    "entry = tk.Entry(entry_frame, font=(\"Arial\", 12), bd=0)\n",
    "entry.grid(row=0, column=0, sticky=\"ew\", padx=(10, 0), pady=10)\n",
    "entry.bind(\"<Return>\", lambda event: send_message())  # Enter to send\n",
    "\n",
    "send_btn = tk.Button(entry_frame, text=\"➤\", command=send_message,\n",
    "                     font=(\"Arial\", 14), bg=\"#FFFFFF\", fg=\"#5C6BC0\", bd=0)\n",
    "send_btn.grid(row=0, column=1, sticky=\"e\", padx=10, pady=10)\n",
    "\n",
    "# ==== Start App ====\n",
    "root.mainloop()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb38187bd56990a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
